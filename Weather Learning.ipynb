{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs and outptus are the maps downloaded from https://www.nnvl.noaa.gov/view/globaldata.html\n",
    "# The inputs to the model are Land Surface Temperature, Moisture and Longwave Energy. \n",
    "# The output is the average precipitation map. \n",
    "# All maps are in the average weekly format. The idea is to find correlation between the input maps and \n",
    "# the output. \n",
    "\n",
    "input_dirs = ['./grayscale/Land_Temp/', './grayscale/Moisture/', './grayscale/Longwave_Energy/'];\n",
    "input_labels = ['land', 'moisture', 'longwave_energy'];\n",
    "input_dict = {}\n",
    "\n",
    "# Process the input : input_metrics will contain a 2048 x 4096 image with 9 channels (corresponding to 3 inputs)\n",
    "\n",
    "for i, path in enumerate(input_dirs):\n",
    "    label = input_labels[i];\n",
    "    input_dict[label] = [];\n",
    "    \n",
    "    for file in sorted(os.listdir(path)):\n",
    "        input_dict[label].append(sp.ndimage.imread(path + file))\n",
    "\n",
    "input_metrics = input_dict[input_labels[0]];\n",
    "for label in (input_labels[1:]):\n",
    "    input_metrics = np.concatenate((input_metrics, input_dict[label]), axis=3);\n",
    "\n",
    "# Process the output : output_metric will contain a 2048 x 4096 image with 3 channels\n",
    "output_metric = [];\n",
    "path = './grayscale/Rain/'\n",
    "for file in sorted(os.listdir(path)):\n",
    "    output_metric.append(sp.ndimage.imread(path + file))\n",
    "\n",
    "output_metric = np.array(output_metric);\n",
    "\n",
    "samples = input_metrics.shape[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to obtain the next batch based on the input size and the batch size\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    idx = (np.random.randint(samples,size=(batch_size,)));\n",
    "    return [input_metrics[idx], output_metric[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple CNN model that looks like an auto-encoder. This is the section to change for a new model.\n",
    "\n",
    "def conv_net(x, weights, biases):\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, 2048, 4096, 9], name='reshape_x');\n",
    "    x = tf.cast(x, tf.float32) \n",
    "    \n",
    "    # Encoder\n",
    "    # 2048x4096x9 -> 1024x2048x32\n",
    "    conv1 = tf.nn.relu(tf.contrib.layers.conv2d(x, 32, [8, 8], stride=2, padding='SAME'))\n",
    "    # 1024x2048x32 -> 512x1024x16\n",
    "    conv2 = tf.nn.relu(tf.contrib.layers.conv2d(conv1, 16, [8, 8], stride=2, padding='SAME'))\n",
    "    # 512x1024x16 -> 256x512x8\n",
    "    conv3 = tf.nn.relu(tf.contrib.layers.conv2d(conv2, 8, [8, 8], stride=2, padding='SAME'))\n",
    "    \n",
    "    # Decoder\n",
    "    # 256x512x8 -> 512x1024x16\n",
    "    conv4 = tf.nn.relu(tf.contrib.layers.conv2d_transpose(conv3, 16, [8, 8], stride=2, padding='SAME'))\n",
    "    # 512x1024x16 -> 1024x2048x32\n",
    "    conv5 = tf.nn.relu(tf.contrib.layers.conv2d_transpose(conv4, 32, [8, 8], stride=2, padding='SAME'))\n",
    "    # 1024x2048x32 -> 2048x4096x3\n",
    "    conv6 = tf.nn.relu(tf.contrib.layers.conv2d_transpose(conv5, 3, [8, 8], stride=2, padding='SAME'))\n",
    "    \n",
    "    return conv6;\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape=(None,2048,4096,3));\n",
    "X = tf.placeholder(tf.float32, shape=[None,2048,4096,9]);\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1);\n",
    "\n",
    "out = conv_net(X, weights, biases);\n",
    "loss_op = tf.reduce_sum(tf.multiply(Y-out,Y-out));\n",
    "train_op = optimizer.minimize(loss_op);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100;\n",
    "batch_size = 1;\n",
    "num_batches = int(samples / batch_size);\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        for i in range(num_batches):\n",
    "            x_train, y_train = next_batch(batch_size);\n",
    "            sess.run([train_op], feed_dict={X: x_train, Y: y_train});            \n",
    "        \n",
    "        loss = sess.run([loss_op], feed_dict={X: x_train, Y: y_train});\n",
    "        print(epoch, loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
